{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import HuichuanFlow as hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 获取同心圆状分布的数据，X的每行包含两个特征，y是1/0类别标签\n",
    "X, y = make_circles(200, noise=0.1, factor=0.2)\n",
    "y = y * 2 - 1  # 将标签转化为1/-1\n",
    "\n",
    "# 是否使用二次项\n",
    "use_quadratic = True\n",
    "\n",
    "# 一次项，2维向量（2x1矩阵）\n",
    "x1 = hf.core.Variable(dim=(2, 1), init=False, trainable=False)\n",
    "\n",
    "# 标签\n",
    "label = hf.core.Variable(dim=(1, 1), init=False, trainable=False)\n",
    "\n",
    "# 偏置\n",
    "b = hf.core.Variable(dim=(1, 1), init=True, trainable=True)\n",
    "\n",
    "# 根据是否使用二次项区别处理\n",
    "if use_quadratic:\n",
    "\n",
    "    # 将一次项与自己的转置相乘，得到二次项2x2矩阵，再转成4维向量（4x1矩阵）\n",
    "    x2 = hf.ops.Reshape(\n",
    "            hf.ops.MatMul(x1, hf.ops.Reshape(x1, shape=(1, 2))),\n",
    "            shape=(4, 1)\n",
    "            )\n",
    "\n",
    "    # 将一次和二次项连接成6维向量（6x1矩阵）\n",
    "    x = hf.ops.Concat(x1, x2)\n",
    "    \n",
    "    # 权值向量是6维（1x6矩阵）\n",
    "    w = hf.core.Variable(dim=(1, 6), init=True, trainable=True)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # 特征向量就是一次项\n",
    "    x = x1\n",
    "    \n",
    "    # 权值向量是2维（1x2矩阵）\n",
    "    w = hf.core.Variable(dim=(1, 2), init=True, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性部分\n",
    "output = hf.ops.Add(hf.ops.MatMul(w, x), b)\n",
    "\n",
    "# 预测概率\n",
    "predict = hf.ops.Logistic(output)\n",
    "\n",
    "# 损失函数\n",
    "loss = hf.ops.loss.LogLoss(hf.ops.MatMul(label, output))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = hf.optimizer.Adam(hf.default_graph, loss, learning_rate)\n",
    "accuracy = hf.ops.metrics.Accuracy(output, label)\n",
    "precision = hf.ops.metrics.Precision(output, label)\n",
    "recall = hf.ops.metrics.Recall(output, label)\n",
    "auc = hf.ops.metrics.ROC_AUC(output, label)\n",
    "\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy: 0.895\n",
      "epoch: 2, accuracy: 0.930\n",
      "epoch: 3, accuracy: 0.920\n",
      "epoch: 4, accuracy: 0.915\n",
      "epoch: 5, accuracy: 0.915\n",
      "epoch: 6, accuracy: 0.925\n",
      "epoch: 7, accuracy: 0.945\n",
      "epoch: 8, accuracy: 0.950\n",
      "epoch: 9, accuracy: 0.955\n",
      "epoch: 10, accuracy: 0.965\n",
      "epoch: 11, accuracy: 0.970\n",
      "epoch: 12, accuracy: 0.975\n",
      "epoch: 13, accuracy: 0.975\n",
      "epoch: 14, accuracy: 0.980\n",
      "epoch: 15, accuracy: 0.990\n",
      "epoch: 16, accuracy: 0.990\n",
      "epoch: 17, accuracy: 0.990\n",
      "epoch: 18, accuracy: 0.995\n",
      "epoch: 19, accuracy: 0.995\n",
      "epoch: 20, accuracy: 0.995\n",
      "epoch: 21, accuracy: 0.995\n",
      "epoch: 22, accuracy: 0.995\n",
      "epoch: 23, accuracy: 0.995\n",
      "epoch: 24, accuracy: 0.995\n",
      "epoch: 25, accuracy: 0.995\n",
      "epoch: 26, accuracy: 0.995\n",
      "epoch: 27, accuracy: 0.995\n",
      "epoch: 28, accuracy: 0.995\n",
      "epoch: 29, accuracy: 0.995\n",
      "epoch: 30, accuracy: 0.995\n",
      "epoch: 31, accuracy: 0.995\n",
      "epoch: 32, accuracy: 0.995\n",
      "epoch: 33, accuracy: 0.995\n",
      "epoch: 34, accuracy: 0.995\n",
      "epoch: 35, accuracy: 0.995\n",
      "epoch: 36, accuracy: 0.995\n",
      "epoch: 37, accuracy: 0.995\n",
      "epoch: 38, accuracy: 0.995\n",
      "epoch: 39, accuracy: 0.995\n",
      "epoch: 40, accuracy: 0.995\n",
      "epoch: 41, accuracy: 0.995\n",
      "epoch: 42, accuracy: 0.995\n",
      "epoch: 43, accuracy: 1.000\n",
      "epoch: 44, accuracy: 1.000\n",
      "epoch: 45, accuracy: 1.000\n",
      "epoch: 46, accuracy: 1.000\n",
      "epoch: 47, accuracy: 1.000\n",
      "epoch: 48, accuracy: 1.000\n",
      "epoch: 49, accuracy: 1.000\n",
      "epoch: 50, accuracy: 1.000\n",
      "epoch: 51, accuracy: 1.000\n",
      "epoch: 52, accuracy: 1.000\n",
      "epoch: 53, accuracy: 1.000\n",
      "epoch: 54, accuracy: 1.000\n",
      "epoch: 55, accuracy: 1.000\n",
      "epoch: 56, accuracy: 1.000\n",
      "epoch: 57, accuracy: 1.000\n",
      "epoch: 58, accuracy: 1.000\n",
      "epoch: 59, accuracy: 1.000\n",
      "epoch: 60, accuracy: 1.000\n",
      "epoch: 61, accuracy: 1.000\n",
      "epoch: 62, accuracy: 1.000\n",
      "epoch: 63, accuracy: 1.000\n",
      "epoch: 64, accuracy: 1.000\n",
      "epoch: 65, accuracy: 1.000\n",
      "epoch: 66, accuracy: 1.000\n",
      "epoch: 67, accuracy: 1.000\n",
      "epoch: 68, accuracy: 1.000\n",
      "epoch: 69, accuracy: 1.000\n",
      "epoch: 70, accuracy: 1.000\n",
      "epoch: 71, accuracy: 1.000\n",
      "epoch: 72, accuracy: 1.000\n",
      "epoch: 73, accuracy: 1.000\n",
      "epoch: 74, accuracy: 1.000\n",
      "epoch: 75, accuracy: 1.000\n",
      "epoch: 76, accuracy: 1.000\n",
      "epoch: 77, accuracy: 1.000\n",
      "epoch: 78, accuracy: 1.000\n",
      "epoch: 79, accuracy: 1.000\n",
      "epoch: 80, accuracy: 1.000\n",
      "epoch: 81, accuracy: 1.000\n",
      "epoch: 82, accuracy: 1.000\n",
      "epoch: 83, accuracy: 1.000\n",
      "epoch: 84, accuracy: 1.000\n",
      "epoch: 85, accuracy: 1.000\n",
      "epoch: 86, accuracy: 1.000\n",
      "epoch: 87, accuracy: 1.000\n",
      "epoch: 88, accuracy: 1.000\n",
      "epoch: 89, accuracy: 1.000\n",
      "epoch: 90, accuracy: 1.000\n",
      "epoch: 91, accuracy: 1.000\n",
      "epoch: 92, accuracy: 1.000\n",
      "epoch: 93, accuracy: 1.000\n",
      "epoch: 94, accuracy: 1.000\n",
      "epoch: 95, accuracy: 1.000\n",
      "epoch: 96, accuracy: 1.000\n",
      "epoch: 97, accuracy: 1.000\n",
      "epoch: 98, accuracy: 1.000\n",
      "epoch: 99, accuracy: 1.000\n",
      "epoch: 100, accuracy: 1.000\n",
      "epoch: 101, accuracy: 1.000\n",
      "epoch: 102, accuracy: 1.000\n",
      "epoch: 103, accuracy: 1.000\n",
      "epoch: 104, accuracy: 1.000\n",
      "epoch: 105, accuracy: 1.000\n",
      "epoch: 106, accuracy: 1.000\n",
      "epoch: 107, accuracy: 1.000\n",
      "epoch: 108, accuracy: 1.000\n",
      "epoch: 109, accuracy: 1.000\n",
      "epoch: 110, accuracy: 1.000\n",
      "epoch: 111, accuracy: 1.000\n",
      "epoch: 112, accuracy: 1.000\n",
      "epoch: 113, accuracy: 1.000\n",
      "epoch: 114, accuracy: 1.000\n",
      "epoch: 115, accuracy: 1.000\n",
      "epoch: 116, accuracy: 1.000\n",
      "epoch: 117, accuracy: 1.000\n",
      "epoch: 118, accuracy: 1.000\n",
      "epoch: 119, accuracy: 1.000\n",
      "epoch: 120, accuracy: 1.000\n",
      "epoch: 121, accuracy: 1.000\n",
      "epoch: 122, accuracy: 1.000\n",
      "epoch: 123, accuracy: 1.000\n",
      "epoch: 124, accuracy: 1.000\n",
      "epoch: 125, accuracy: 1.000\n",
      "epoch: 126, accuracy: 1.000\n",
      "epoch: 127, accuracy: 1.000\n",
      "epoch: 128, accuracy: 1.000\n",
      "epoch: 129, accuracy: 1.000\n",
      "epoch: 130, accuracy: 1.000\n",
      "epoch: 131, accuracy: 1.000\n",
      "epoch: 132, accuracy: 1.000\n",
      "epoch: 133, accuracy: 1.000\n",
      "epoch: 134, accuracy: 1.000\n",
      "epoch: 135, accuracy: 1.000\n",
      "epoch: 136, accuracy: 1.000\n",
      "epoch: 137, accuracy: 1.000\n",
      "epoch: 138, accuracy: 1.000\n",
      "epoch: 139, accuracy: 1.000\n",
      "epoch: 140, accuracy: 1.000\n",
      "epoch: 141, accuracy: 1.000\n",
      "epoch: 142, accuracy: 1.000\n",
      "epoch: 143, accuracy: 1.000\n",
      "epoch: 144, accuracy: 1.000\n",
      "epoch: 145, accuracy: 1.000\n",
      "epoch: 146, accuracy: 1.000\n",
      "epoch: 147, accuracy: 1.000\n",
      "epoch: 148, accuracy: 1.000\n",
      "epoch: 149, accuracy: 1.000\n",
      "epoch: 150, accuracy: 1.000\n",
      "epoch: 151, accuracy: 1.000\n",
      "epoch: 152, accuracy: 1.000\n",
      "epoch: 153, accuracy: 1.000\n",
      "epoch: 154, accuracy: 1.000\n",
      "epoch: 155, accuracy: 1.000\n",
      "epoch: 156, accuracy: 1.000\n",
      "epoch: 157, accuracy: 1.000\n",
      "epoch: 158, accuracy: 1.000\n",
      "epoch: 159, accuracy: 1.000\n",
      "epoch: 160, accuracy: 1.000\n",
      "epoch: 161, accuracy: 1.000\n",
      "epoch: 162, accuracy: 1.000\n",
      "epoch: 163, accuracy: 1.000\n",
      "epoch: 164, accuracy: 1.000\n",
      "epoch: 165, accuracy: 1.000\n",
      "epoch: 166, accuracy: 1.000\n",
      "epoch: 167, accuracy: 1.000\n",
      "epoch: 168, accuracy: 1.000\n",
      "epoch: 169, accuracy: 1.000\n",
      "epoch: 170, accuracy: 1.000\n",
      "epoch: 171, accuracy: 1.000\n",
      "epoch: 172, accuracy: 1.000\n",
      "epoch: 173, accuracy: 1.000\n",
      "epoch: 174, accuracy: 1.000\n",
      "epoch: 175, accuracy: 1.000\n",
      "epoch: 176, accuracy: 1.000\n",
      "epoch: 177, accuracy: 1.000\n",
      "epoch: 178, accuracy: 1.000\n",
      "epoch: 179, accuracy: 1.000\n",
      "epoch: 180, accuracy: 1.000\n",
      "epoch: 181, accuracy: 1.000\n",
      "epoch: 182, accuracy: 1.000\n",
      "epoch: 183, accuracy: 1.000\n",
      "epoch: 184, accuracy: 1.000\n",
      "epoch: 185, accuracy: 1.000\n",
      "epoch: 186, accuracy: 1.000\n",
      "epoch: 187, accuracy: 1.000\n",
      "epoch: 188, accuracy: 1.000\n",
      "epoch: 189, accuracy: 1.000\n",
      "epoch: 190, accuracy: 1.000\n",
      "epoch: 191, accuracy: 1.000\n",
      "epoch: 192, accuracy: 1.000\n",
      "epoch: 193, accuracy: 1.000\n",
      "epoch: 194, accuracy: 1.000\n",
      "epoch: 195, accuracy: 1.000\n",
      "epoch: 196, accuracy: 1.000\n",
      "epoch: 197, accuracy: 1.000\n",
      "epoch: 198, accuracy: 1.000\n",
      "epoch: 199, accuracy: 1.000\n",
      "epoch: 200, accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    batch_count = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        \n",
    "        x1.set_value(np.mat(X[i]).T)\n",
    "        label.set_value(np.mat(y[i]))\n",
    "        \n",
    "        optimizer.one_step()\n",
    "        \n",
    "        batch_count += 1\n",
    "        \n",
    "        if batch_count >= batch_size:\n",
    "            optimizer.update()\n",
    "            batch_count = 0\n",
    "\n",
    "    pred = []\n",
    "    for i in range(len(X)):\n",
    "                \n",
    "        x1.set_value(np.mat(X[i]).T)\n",
    "        label.set_value(np.mat(y[i]))\n",
    "        \n",
    "        predict.forward()\n",
    "        pred.append(predict.value[0, 0])\n",
    "            \n",
    "    pred = (np.array(pred) > 0.5).astype(np.int) * 2 - 1\n",
    "    \n",
    "    accuracy = (y == pred).astype(np.int).sum() / len(X)\n",
    "    print(\"epoch: {:d}, accuracy: {:.3f}\".format(epoch + 1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
