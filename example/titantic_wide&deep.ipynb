{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy: 0.705\n",
      "epoch: 2, accuracy: 0.773\n",
      "epoch: 3, accuracy: 0.796\n",
      "epoch: 4, accuracy: 0.789\n",
      "epoch: 5, accuracy: 0.787\n",
      "epoch: 6, accuracy: 0.788\n",
      "epoch: 7, accuracy: 0.790\n",
      "epoch: 8, accuracy: 0.795\n",
      "epoch: 9, accuracy: 0.797\n",
      "epoch: 10, accuracy: 0.797\n",
      "epoch: 11, accuracy: 0.795\n",
      "epoch: 12, accuracy: 0.799\n",
      "epoch: 13, accuracy: 0.801\n",
      "epoch: 14, accuracy: 0.801\n",
      "epoch: 15, accuracy: 0.801\n",
      "epoch: 16, accuracy: 0.802\n",
      "epoch: 17, accuracy: 0.802\n",
      "epoch: 18, accuracy: 0.802\n",
      "epoch: 19, accuracy: 0.802\n",
      "epoch: 20, accuracy: 0.799\n",
      "epoch: 21, accuracy: 0.802\n",
      "epoch: 22, accuracy: 0.801\n",
      "epoch: 23, accuracy: 0.799\n",
      "epoch: 24, accuracy: 0.798\n",
      "epoch: 25, accuracy: 0.799\n",
      "epoch: 26, accuracy: 0.800\n",
      "epoch: 27, accuracy: 0.800\n",
      "epoch: 28, accuracy: 0.801\n",
      "epoch: 29, accuracy: 0.800\n",
      "epoch: 30, accuracy: 0.799\n",
      "epoch: 31, accuracy: 0.800\n",
      "epoch: 32, accuracy: 0.800\n",
      "epoch: 33, accuracy: 0.799\n",
      "epoch: 34, accuracy: 0.799\n",
      "epoch: 35, accuracy: 0.799\n",
      "epoch: 36, accuracy: 0.799\n",
      "epoch: 37, accuracy: 0.799\n",
      "epoch: 38, accuracy: 0.799\n",
      "epoch: 39, accuracy: 0.799\n",
      "epoch: 40, accuracy: 0.799\n",
      "epoch: 41, accuracy: 0.799\n",
      "epoch: 42, accuracy: 0.799\n",
      "epoch: 43, accuracy: 0.799\n",
      "epoch: 44, accuracy: 0.799\n",
      "epoch: 45, accuracy: 0.799\n",
      "epoch: 46, accuracy: 0.798\n",
      "epoch: 47, accuracy: 0.799\n",
      "epoch: 48, accuracy: 0.799\n",
      "epoch: 49, accuracy: 0.799\n",
      "epoch: 50, accuracy: 0.799\n",
      "epoch: 51, accuracy: 0.799\n",
      "epoch: 52, accuracy: 0.799\n",
      "epoch: 53, accuracy: 0.799\n",
      "epoch: 54, accuracy: 0.799\n",
      "epoch: 55, accuracy: 0.799\n",
      "epoch: 56, accuracy: 0.799\n",
      "epoch: 57, accuracy: 0.799\n",
      "epoch: 58, accuracy: 0.799\n",
      "epoch: 59, accuracy: 0.799\n",
      "epoch: 60, accuracy: 0.799\n",
      "epoch: 61, accuracy: 0.799\n",
      "epoch: 62, accuracy: 0.799\n",
      "epoch: 63, accuracy: 0.799\n",
      "epoch: 64, accuracy: 0.799\n",
      "epoch: 65, accuracy: 0.799\n",
      "epoch: 66, accuracy: 0.799\n",
      "epoch: 67, accuracy: 0.799\n",
      "epoch: 68, accuracy: 0.799\n",
      "epoch: 69, accuracy: 0.799\n",
      "epoch: 70, accuracy: 0.802\n",
      "epoch: 71, accuracy: 0.799\n",
      "epoch: 72, accuracy: 0.798\n",
      "epoch: 73, accuracy: 0.799\n",
      "epoch: 74, accuracy: 0.799\n",
      "epoch: 75, accuracy: 0.799\n",
      "epoch: 76, accuracy: 0.799\n",
      "epoch: 77, accuracy: 0.799\n",
      "epoch: 78, accuracy: 0.799\n",
      "epoch: 79, accuracy: 0.799\n",
      "epoch: 80, accuracy: 0.799\n",
      "epoch: 81, accuracy: 0.799\n",
      "epoch: 82, accuracy: 0.797\n",
      "epoch: 83, accuracy: 0.797\n",
      "epoch: 84, accuracy: 0.797\n",
      "epoch: 85, accuracy: 0.797\n",
      "epoch: 86, accuracy: 0.797\n",
      "epoch: 87, accuracy: 0.797\n",
      "epoch: 88, accuracy: 0.797\n",
      "epoch: 89, accuracy: 0.797\n",
      "epoch: 90, accuracy: 0.797\n",
      "epoch: 91, accuracy: 0.797\n",
      "epoch: 92, accuracy: 0.800\n",
      "epoch: 93, accuracy: 0.799\n",
      "epoch: 94, accuracy: 0.798\n",
      "epoch: 95, accuracy: 0.798\n",
      "epoch: 96, accuracy: 0.798\n",
      "epoch: 97, accuracy: 0.798\n",
      "epoch: 98, accuracy: 0.798\n",
      "epoch: 99, accuracy: 0.798\n",
      "epoch: 100, accuracy: 0.798\n",
      "epoch: 101, accuracy: 0.797\n",
      "epoch: 102, accuracy: 0.797\n",
      "epoch: 103, accuracy: 0.797\n",
      "epoch: 104, accuracy: 0.797\n",
      "epoch: 105, accuracy: 0.797\n",
      "epoch: 106, accuracy: 0.797\n",
      "epoch: 107, accuracy: 0.797\n",
      "epoch: 108, accuracy: 0.797\n",
      "epoch: 109, accuracy: 0.798\n",
      "epoch: 110, accuracy: 0.798\n",
      "epoch: 111, accuracy: 0.798\n",
      "epoch: 112, accuracy: 0.798\n",
      "epoch: 113, accuracy: 0.798\n",
      "epoch: 114, accuracy: 0.797\n",
      "epoch: 115, accuracy: 0.797\n",
      "epoch: 116, accuracy: 0.797\n",
      "epoch: 117, accuracy: 0.797\n",
      "epoch: 118, accuracy: 0.797\n",
      "epoch: 119, accuracy: 0.797\n",
      "epoch: 120, accuracy: 0.797\n",
      "epoch: 121, accuracy: 0.797\n",
      "epoch: 122, accuracy: 0.797\n",
      "epoch: 123, accuracy: 0.797\n",
      "epoch: 124, accuracy: 0.797\n",
      "epoch: 125, accuracy: 0.797\n",
      "epoch: 126, accuracy: 0.797\n",
      "epoch: 127, accuracy: 0.797\n",
      "epoch: 128, accuracy: 0.797\n",
      "epoch: 129, accuracy: 0.797\n",
      "epoch: 130, accuracy: 0.797\n",
      "epoch: 131, accuracy: 0.797\n",
      "epoch: 132, accuracy: 0.797\n",
      "epoch: 133, accuracy: 0.797\n",
      "epoch: 134, accuracy: 0.797\n",
      "epoch: 135, accuracy: 0.797\n",
      "epoch: 136, accuracy: 0.797\n",
      "epoch: 137, accuracy: 0.797\n",
      "epoch: 138, accuracy: 0.797\n",
      "epoch: 139, accuracy: 0.797\n",
      "epoch: 140, accuracy: 0.797\n",
      "epoch: 141, accuracy: 0.797\n",
      "epoch: 142, accuracy: 0.797\n",
      "epoch: 143, accuracy: 0.797\n",
      "epoch: 144, accuracy: 0.797\n",
      "epoch: 145, accuracy: 0.797\n",
      "epoch: 146, accuracy: 0.797\n",
      "epoch: 147, accuracy: 0.797\n",
      "epoch: 148, accuracy: 0.797\n",
      "epoch: 149, accuracy: 0.797\n",
      "epoch: 150, accuracy: 0.797\n",
      "epoch: 151, accuracy: 0.797\n",
      "epoch: 152, accuracy: 0.797\n",
      "epoch: 153, accuracy: 0.797\n",
      "epoch: 154, accuracy: 0.797\n",
      "epoch: 155, accuracy: 0.797\n",
      "epoch: 156, accuracy: 0.797\n",
      "epoch: 157, accuracy: 0.797\n",
      "epoch: 158, accuracy: 0.797\n",
      "epoch: 159, accuracy: 0.797\n",
      "epoch: 160, accuracy: 0.797\n",
      "epoch: 161, accuracy: 0.797\n",
      "epoch: 162, accuracy: 0.797\n",
      "epoch: 163, accuracy: 0.797\n",
      "epoch: 164, accuracy: 0.797\n",
      "epoch: 165, accuracy: 0.797\n",
      "epoch: 166, accuracy: 0.797\n",
      "epoch: 167, accuracy: 0.797\n",
      "epoch: 168, accuracy: 0.797\n",
      "epoch: 169, accuracy: 0.797\n",
      "epoch: 170, accuracy: 0.797\n",
      "epoch: 171, accuracy: 0.797\n",
      "epoch: 172, accuracy: 0.797\n",
      "epoch: 173, accuracy: 0.797\n",
      "epoch: 174, accuracy: 0.797\n",
      "epoch: 175, accuracy: 0.797\n",
      "epoch: 176, accuracy: 0.797\n",
      "epoch: 177, accuracy: 0.797\n",
      "epoch: 178, accuracy: 0.797\n",
      "epoch: 179, accuracy: 0.797\n",
      "epoch: 180, accuracy: 0.797\n",
      "epoch: 181, accuracy: 0.797\n",
      "epoch: 182, accuracy: 0.797\n",
      "epoch: 183, accuracy: 0.797\n",
      "epoch: 184, accuracy: 0.797\n",
      "epoch: 185, accuracy: 0.797\n",
      "epoch: 186, accuracy: 0.797\n",
      "epoch: 187, accuracy: 0.797\n",
      "epoch: 188, accuracy: 0.797\n",
      "epoch: 189, accuracy: 0.797\n",
      "epoch: 190, accuracy: 0.797\n",
      "epoch: 191, accuracy: 0.797\n",
      "epoch: 192, accuracy: 0.797\n",
      "epoch: 193, accuracy: 0.797\n",
      "epoch: 194, accuracy: 0.797\n",
      "epoch: 195, accuracy: 0.797\n",
      "epoch: 196, accuracy: 0.797\n",
      "epoch: 197, accuracy: 0.797\n",
      "epoch: 198, accuracy: 0.797\n",
      "epoch: 199, accuracy: 0.797\n",
      "epoch: 200, accuracy: 0.797\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import HuichuanFlow as ms\n",
    "\n",
    "# 读取数据，去掉无用列\n",
    "data = pd.read_csv(\"../data/titanic.csv\").drop([\"PassengerId\", \n",
    "                  \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# 构造编码类\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# 对类别型特征做One-Hot编码\n",
    "Pclass = ohe.fit_transform(le.fit_transform(data[\"Pclass\"].fillna(0)).reshape(-1, 1))\n",
    "Sex = ohe.fit_transform(le.fit_transform(data[\"Sex\"].fillna(\"\")).reshape(-1, 1))\n",
    "Embarked = ohe.fit_transform(le.fit_transform(data[\"Embarked\"].fillna(\"\")).reshape(-1, 1))\n",
    "\n",
    "# 组合特征列\n",
    "features = np.concatenate([Pclass,\n",
    "                           Sex,\n",
    "                           data[[\"Age\"]].fillna(0),\n",
    "                           data[[\"SibSp\"]].fillna(0),\n",
    "                           data[[\"Parch\"]].fillna(0),\n",
    "                           data[[\"Fare\"]].fillna(0),\n",
    "                           Embarked\n",
    "                           ], axis=1)\n",
    "\n",
    "# 标签\n",
    "labels = data[\"Survived\"].values * 2 - 1\n",
    "\n",
    "# 特征维数\n",
    "dimension = features.shape[1]\n",
    "\n",
    "# 嵌入向量维度\n",
    "k = 2\n",
    "\n",
    "# 一次项\n",
    "x = ms.core.Variable(dim=(dimension, 1), init=False, trainable=False)\n",
    "\n",
    "# 三个类别类特征的三套One-Hot\n",
    "x_Pclass = ms.core.Variable(dim=(Pclass.shape[1], 1), init=False, trainable=False)\n",
    "x_Sex = ms.core.Variable(dim=(Sex.shape[1], 1), init=False, trainable=False)\n",
    "x_Embarked = ms.core.Variable(dim=(Embarked.shape[1], 1), init=False, trainable=False)\n",
    "\n",
    "\n",
    "# 标签\n",
    "label = ms.core.Variable(dim=(1, 1), init=False, trainable=False)\n",
    "\n",
    "# 一次项权值向量\n",
    "w = ms.core.Variable(dim=(1, dimension), init=True, trainable=True)\n",
    "\n",
    "# 类别类特征的嵌入矩阵\n",
    "E_Pclass = ms.core.Variable(dim=(k, Pclass.shape[1]), init=True, trainable=True)\n",
    "E_Sex = ms.core.Variable(dim=(k, Sex.shape[1]), init=True, trainable=True)\n",
    "E_Embarked = ms.core.Variable(dim=(k, Embarked.shape[1]), init=True, trainable=True)\n",
    "\n",
    "# 偏置\n",
    "b = ms.core.Variable(dim=(1, 1), init=True, trainable=True)\n",
    "\n",
    "\n",
    "# Wide部分\n",
    "wide = ms.ops.MatMul(w, x)\n",
    "\n",
    "\n",
    "# Deep部分，三个嵌入向量\n",
    "embedding_Pclass = ms.ops.MatMul(E_Pclass, x_Pclass)\n",
    "embedding_Sex = ms.ops.MatMul(E_Sex, x_Sex)\n",
    "embedding_Embarked = ms.ops.MatMul(E_Embarked, x_Embarked)\n",
    "\n",
    "# 将三个嵌入向量连接在一起\n",
    "embedding = ms.ops.Concat(\n",
    "        embedding_Pclass,\n",
    "        embedding_Sex,\n",
    "        embedding_Embarked\n",
    "        )\n",
    "\n",
    "# 第一隐藏层\n",
    "hidden_1 = ms.layer.fc(embedding, 3 * k, 8, \"ReLU\")\n",
    "\n",
    "# 第二隐藏层\n",
    "hidden_2 = ms.layer.fc(hidden_1, 8, 4, \"ReLU\")\n",
    "\n",
    "# 输出层\n",
    "deep = ms.layer.fc(hidden_2, 4, 1, None)\n",
    "\n",
    "# 输出\n",
    "output = ms.ops.Add(wide, deep, b)\n",
    "\n",
    "# 预测概率\n",
    "predict = ms.ops.Logistic(output)\n",
    "\n",
    "# 损失函数\n",
    "loss = ms.ops.loss.LogLoss(ms.ops.Multiply(label, output))\n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = ms.optimizer.Adam(ms.default_graph, loss, learning_rate)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    batch_count = 0   \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        x.set_value(np.mat(features[i]).T)\n",
    "        \n",
    "        # 从特征中选择各段One-Hot编码\n",
    "        x_Pclass.set_value(np.mat(features[i, :3]).T)\n",
    "        x_Sex.set_value(np.mat(features[i, 3:5]).T)\n",
    "        x_Embarked.set_value(np.mat(features[i, 9:]).T)\n",
    "        \n",
    "        label.set_value(np.mat(labels[i]))\n",
    "        \n",
    "        optimizer.one_step()\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count >= batch_size:\n",
    "            \n",
    "            optimizer.update()\n",
    "            batch_count = 0\n",
    "        \n",
    "\n",
    "    pred = []\n",
    "    for i in range(len(features)):\n",
    "                \n",
    "        x.set_value(np.mat(features[i]).T)\n",
    "        x_Pclass.set_value(np.mat(features[i, :3]).T)\n",
    "        x_Sex.set_value(np.mat(features[i, 3:5]).T)\n",
    "        x_Embarked.set_value(np.mat(features[i, 9:]).T)\n",
    "        \n",
    "        predict.forward()\n",
    "        pred.append(predict.value[0, 0])\n",
    "            \n",
    "    pred = (np.array(pred) > 0.5).astype(np.int) * 2 - 1\n",
    "    accuracy = (labels == pred).astype(np.int).sum() / len(features)\n",
    "       \n",
    "    print(\"epoch: {:d}, accuracy: {:.3f}\".format(epoch + 1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
